import os
import argparse

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
import torchvision
import torchvision.transforms as transforms
from tqdm.auto import tqdm

import metrics_platonic as metrics
import resnet_arch
import utils


def load_resnet_from_ckpt(ckpt_path: str, device: torch.device):
    ckpt = torch.load(ckpt_path, map_location=device)
    model = resnet_arch.resnet_18_cifar()
    model.load_state_dict(ckpt["state_dict"], strict=True)
    model.to(device).eval()
    return model


def build_cifar_loader(
    split: str,
    runs_dir: str,
    split_seed: int,
    batch_size: int,
    num_workers: int,
    data_root: str = "./data",
):
    # deterministic transform (do NOT use train-time random aug for representation comparison)
    tfm = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.49139968, 0.48215841, 0.44653091),
                             (0.24703223, 0.24348513, 0.26158784)),
    ])

    if split == "test":
        ds = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=tfm)
    elif split in ("train", "val"):
        full = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=tfm)
        split_path = os.path.join(runs_dir, f"split_indices_seed{split_seed}.pt")
        idx = torch.load(split_path)
        indices = idx["train_indices"] if split == "train" else idx["val_indices"]
        ds = Subset(full, indices)
    else:
        raise ValueError(f"Unknown split: {split}")

    return DataLoader(
        ds,
        batch_size=batch_size,
        shuffle=False,  # critical: keep identical ordering across models
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
    )


def extract_layer_features(
    model: torch.nn.Module,
    loader: DataLoader,
    device: torch.device,
    layers: list[str],
    pool: str = "avg",          # avg = spatial mean for conv outputs
    max_samples: int | None = None,
):
    named_modules = dict(model.named_modules())
    for ln in layers:
        if ln not in named_modules:
            raise ValueError(f"Layer '{ln}' not found. Available: {list(named_modules.keys())[:20]} ...")

    buckets = {ln: [] for ln in layers}
    handles = []

    def make_hook(layer_name: str):
        def hook(_module, _inp, out):
            x = out
            if isinstance(x, (tuple, list)):
                x = x[0]

            if x.ndim == 4:  # B,C,H,W
                if pool == "avg":
                    x = x.mean(dim=(2, 3))
                elif pool == "flatten":
                    x = x.flatten(1)
                else:
                    raise ValueError(f"Unknown pool: {pool}")
            else:
                x = x.flatten(1)

            buckets[layer_name].append(x.detach().cpu())
        return hook

    for ln in layers:
        handles.append(named_modules[ln].register_forward_hook(make_hook(ln)))

    n_seen = 0
    model.eval()
    with torch.no_grad():
        for xb, _yb in tqdm(loader, desc="Extracting features", leave=False):
            xb = xb.to(device)
            _ = model(xb)
            n_seen += xb.size(0)
            if max_samples is not None and n_seen >= max_samples:
                break

    for h in handles:
        h.remove()

    feats = []
    for ln in layers:
        f = torch.cat(buckets[ln], dim=0)
        if max_samples is not None:
            f = f[:max_samples]
        feats.append(f)
    return feats  # list of [N,D] CPU tensors


def prepare_features(feats: torch.Tensor, device: torch.device, q: float = 0.95, exact: bool = False):
    feats = metrics.remove_outliers(feats.float(), q=q, exact=exact)
    return feats.to(device)


def layerwise_scores(
    feats_A: list[torch.Tensor],
    feats_B: list[torch.Tensor],
    device: torch.device,
    metric: str,
    topk: int,
    q: float,
    exact: bool,
    normalize: bool = True,
    pairing: str = "diagonal",   # diagonal or best
):
    if pairing not in ("diagonal", "best"):
        raise ValueError("pairing must be 'diagonal' or 'best'")

    # Move + preprocess once (still modest size at CIFAR10 layer dims)
    A = [prepare_features(f, device=device, q=q, exact=exact) for f in feats_A]
    B = [prepare_features(f, device=device, q=q, exact=exact) for f in feats_B]

    if normalize:
        A = [F.normalize(f, p=2, dim=-1) for f in A]
        B = [F.normalize(f, p=2, dim=-1) for f in B]

    def score(fa, fb):
        kwargs = {"topk": topk} if "knn" in metric else {}
        return metrics.AlignmentMetrics.measure(metric, fa, fb, **kwargs)

    if pairing == "diagonal":
        assert len(A) == len(B), "Diagonal pairing requires same number of layers"
        scores = [score(A[i], B[i]) for i in range(len(A))]
        return np.array(scores, dtype=float), None

    # pairing == "best": search best (i,j)
    best = -1e9
    best_ij = None
    for i in range(len(A)):
        for j in range(len(B)):
            s = score(A[i], B[j])
            if s > best:
                best = s
                best_ij = (i, j)
    return np.array([best], dtype=float), best_ij


def main():
    parser = argparse.ArgumentParser()

    # Provide either explicit checkpoints OR seeds + runs_dir + which
    parser.add_argument("--ckpt_a", type=str, default=None)
    parser.add_argument("--ckpt_b", type=str, default=None)
    parser.add_argument("--seed_a", type=int, default=None)
    parser.add_argument("--seed_b", type=int, default=None)

    parser.add_argument("--runs_dir", type=str, default="./runs_resnet18_cifar10")
    parser.add_argument("--which", type=str, default="best", choices=["best", "final"])

    parser.add_argument("--split", type=str, default="val", choices=["train", "val", "test"])
    parser.add_argument("--split_seed", type=int, default=50)

    parser.add_argument("--batch_size", type=int, default=256)
    parser.add_argument("--num_workers", type=int, default=0)
    parser.add_argument("--max_samples", type=int, default=None)

    parser.add_argument("--layers", nargs="+", default=["layer1", "layer2", "layer3", "layer4", "avgpool"])
    parser.add_argument("--pool", type=str, default="avg", choices=["avg", "flatten"])

    parser.add_argument("--metric", type=str, default="mutual_knn",
                        choices=["mutual_knn", "cka", "unbiased_cka", "cknna"])
    parser.add_argument("--topk", type=int, default=10)
    parser.add_argument("--pairing", type=str, default="diagonal", choices=["diagonal", "best"])

    parser.add_argument("--q", type=float, default=0.95)         # outlier clamp quantile
    parser.add_argument("--exact", action="store_true")          # exact quantile
    parser.add_argument("--no_normalize", action="store_true")   # skip L2 normalize
    parser.add_argument("--output_dir", type=str, default="./results/alignment_resnet")
    args = parser.parse_args()

    device = utils.get_device()

    # Resolve checkpoint paths
    if args.ckpt_a is None or args.ckpt_b is None:
        if args.seed_a is None or args.seed_b is None:
            raise ValueError("Provide either --ckpt_a/--ckpt_b or --seed_a/--seed_b")

        def ckpt_from_seed(seed: int):
            run_dir = os.path.join(args.runs_dir, f"seed_{seed}")
            fname = f"resnet18_cifar10_seed{seed}_{args.which}.pth"
            return os.path.join(run_dir, fname)

        ckpt_a = ckpt_from_seed(args.seed_a)
        ckpt_b = ckpt_from_seed(args.seed_b)
    else:
        ckpt_a, ckpt_b = args.ckpt_a, args.ckpt_b

    assert os.path.exists(ckpt_a), ckpt_a
    assert os.path.exists(ckpt_b), ckpt_b

    # Data
    loader = build_cifar_loader(
        split=args.split,
        runs_dir=args.runs_dir,
        split_seed=args.split_seed,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
    )

    # Models
    model_a = load_resnet_from_ckpt(ckpt_a, device=device)
    model_b = load_resnet_from_ckpt(ckpt_b, device=device)

    # Features
    feats_a = extract_layer_features(model_a, loader, device, layers=args.layers,
                                     pool=args.pool, max_samples=args.max_samples)
    feats_b = extract_layer_features(model_b, loader, device, layers=args.layers,
                                     pool=args.pool, max_samples=args.max_samples)

    # Scores
    scores, best_ij = layerwise_scores(
        feats_a, feats_b,
        device=device,
        metric=args.metric,
        topk=args.topk,
        q=args.q,
        exact=args.exact,
        normalize=(not args.no_normalize),
        pairing=args.pairing,
    )

    # Report
    print(f"ckpt_a: {ckpt_a}")
    print(f"ckpt_b: {ckpt_b}")
    print(f"split: {args.split} | metric: {args.metric} | pairing: {args.pairing}")
    if "knn" in args.metric:
        print(f"topk: {args.topk}")
    if args.pairing == "diagonal":
        for i, ln in enumerate(args.layers):
            print(f"{ln:>10}: {scores[i]:.6f}")
        print(f"{'mean':>10}: {scores.mean():.6f}")
    else:
        print(f"best_score: {scores[0]:.6f} at layers (a,b) = {best_ij}")

    # Save
    os.makedirs(args.output_dir, exist_ok=True)
    base_a = os.path.basename(ckpt_a).replace(".pth", "")
    base_b = os.path.basename(ckpt_b).replace(".pth", "")
    out_name = f"{base_a}__vs__{base_b}__{args.split}__{args.metric}__{args.pairing}.npz"
    out_path = os.path.join(args.output_dir, out_name)

    np.savez(
        out_path,
        scores=scores,
        best_ij=np.array(best_ij) if best_ij is not None else None,
        layers=np.array(args.layers),
        ckpt_a=ckpt_a,
        ckpt_b=ckpt_b,
        split=args.split,
        metric=args.metric,
        topk=args.topk,
        q=args.q,
        pool=args.pool,
        max_samples=args.max_samples,
    )
    print(f"saved: {out_path}")


if __name__ == "__main__":
    main()
